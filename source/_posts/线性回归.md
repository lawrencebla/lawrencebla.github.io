---
title: 线性回归
date: 2018-10-18 18:31:33
tags:
  - 线性回归
---

线性回归主要用于给出对应参数，__预测__ 出对应值的方法。
<!-- more --> 

图中的昆虫鸣叫次数（x）和温度（y）的样本示例图，红点是样本数据：
{% asset_img 01.svg 样本数据 %}

这时候如果需要预测昆虫每分钟叫125次时的温度，就需要用到线性回归：
{% asset_img 02.svg 线性回归 %}
经过线性回归处理后，可以得到一条线（__一个求y的假设函数: h__）

y = w·x + b;

这时不管从图中还是从公式中，都可以很容易得出，当鸣叫次数（x）为125时，温度（y）是多少。

## 损失

判断 __假设函数：h__ 是否合适，由 __损失__ 进行判断。

* 红色箭头表示 __损失__ 。
* 蓝线表示 __h__ 的图形表述。
{% asset_img 03.png 损失 %}
左边的 __损失__ 明显大于右边。

## 损失函数

通常会使用一个 __损失函数：j__ 来表示损失。而线性回归的损失函数可以使用 __均方误差（Mean Squared Error）__。
{% asset_img 04.png 均方误差公式 %}
其中：
* x和y指的是样本的特征值（鸣叫次数）和标签（温度）
* 函数h即为上面提到的 __假设函数h__
* N指的是样本个数

找出函数 __h__ 的最合适值，也就是要找出 __j__ 函数 对于w和b的最小值。
将函数 __h__ 代入函数 __j__ 可以得到最终函数：
j = c1 - c2.w + c3·w^2 + c4·b^2 + c5·w·b
其中：
* c1指所有y的平方的和的平均值
* c2指2·所有x·y的和的平均值
* c3指所有x的平方的和的平均值
* c4指1
* c5指2·所有x的平均值
因为j是基于w和b的函数，即对于同一个 __假设函数h__ ，所有的x和y的样本是一致的，所以c*都可以看作是常量。
最终 __代价函数j__ 是一个2元2次函数，函数图形为碗状:
{% asset_img 05.png 损失函数图形 %}
目标就是不断调整w和b，找到图像的最低点。

## 梯度下降

为了简化公式，将 __b__ 视为0, __代价函数j__ 则为一个2元1次方程：
j = c1 - c2.w + c3·w^2
{% asset_img 08.png 简化损失函数图形 %}

梯度下降是用来计算低点的方法之一：随机选一点，在沿着负梯度的方向前进，最后会找到低点。
{% asset_img 07.svg 梯度下降 %}
若y轴的损失用loss表示，实际上会重复执行如下函数，以找到min(loss)
loss = loss - x
这里有2个问题，1. loss减去的x是什么；2. 函数到什么时候不再执行。

### x是什么
选取点位于最低点的左右，如何保证一致下降而不是上升，可以通过该点的斜率来判断。
* 当Δy/Δw>0的时候，表示Δy/Δw正相关，即想要Δy减小，Δy就要w小
* 当Δy/Δw<0的时候，表示Δy/Δw负相关，即想要Δy减小，Δy就要w加
所以x就是数倍的Δy/Δw，倍数用α表述
x = α · Δy/Δw
w也会不断变化, 与x类似，只不过使用Δw/Δy, 以至于找到最终的w
w = w - α · Δw/Δy

### 什么时候停止
当Δx/Δy为0时，即斜率为0，也就是找到了低点，可以停止了。但是很难达到正好斜率为0的情况，所以一般认为，当斜率变动很小的时候，即为低点。

对于 __代价函数j__ 是二元二次函数的情况，只有一个相对低点，也就是绝对低点。非二元二次方程时，斜率坑不是最低点。
{% asset_img 09.png 局部最低点 %}
这里有2个局部最低点和一个全局最低点

## 选择合适的α
倍数的选择对于梯度下降影响很大，当倍数过小时，由于斜率在不断趋近于0，所以需要很多轮计算才能找到低点。当斜率过大时，可能会在低点附近来回变动，以至于一致找不到低点。

{% asset_img 10.png 合适的学习速率 %}

学习速率的选择，可以尝试0.001、0.01、0.1、1。选择一个最大的学习速率，然后选择一个比它小一点点的学习率，通常能够找到最合适的学习速率来解决我们的问题。

{% asset_img 11.png 不同的学习速率 %}

## 得到最好的假设函数h

为了更好的将导数映射到多元，使用偏微分的形式展示梯度下降函数
w = w - α · ∂J(w)/∂w
当加入b参数时
w = w - α · ∂J(w, b)/∂w
b = b - α · ∂J(w, b)/∂b

将 __代价函数j__ 和 __假设函数h__ 代入公式后， 梯度下降将不断计算新的w和b，已得到假设函数h

代入公式如下图，θ0表示b, θ1表示w，m表示N，__代价函数j__ 公式多除以2，对于结果没有影响。

{% asset_img 12.png 代入j和h公式 %}
{% asset_img 13.png 代入j和h公式 %}
{% asset_img 14.png 代入j和h公式 %}
{% asset_img 15.png 代入j和h公式 %}

_图片来自Andrew Ng在https://www.coursera.org/learn/machine-learning中的讲义_

__至此，就可以求出最合适假设函数h__

---

_以下为代价函数j推导_


{% asset_img 06.png 代价函数推导 %}